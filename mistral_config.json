{
  "llama_cpp": {
    "llamaCppPath": "C:/llama/llama-server.exe",
    "modelPath": "D:/AI-Models/llama/mistral/mistral-7b-instruct-v0.2.Q4_K_M.gguf",
    "chatTemplate": "chatml",
    "port": 8081,
    "ctxSize": 16384,
    "batchSize": 128,
    "ubatchSize": 64,
    "parallel": 1,
    "threads": 6,
    "gpuLayers": 20,
    "cacheK": "q8_0",
    "cacheV": "q8_0", 
    "nPredict": 2048,
    "temp": 0.5,
    "topK": 40,
    "topP": 0.9,
    "repeatPen": 1.08,
    "mirostat": 0,
    "flashAttn": 0
  }
}